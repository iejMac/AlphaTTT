{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "backed-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from model import ZeroTTT\n",
    "from database import DataBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "administrative-pride",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRemember that last index of eigth batch games is 218\\n                            ninth batch games is 248\\n                            10th batch games is 288\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparams:\n",
    "epochs = 1\n",
    "lr = 3e-4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "batch_size=100\n",
    "'''\n",
    "Remember that last index of eigth batch games is 218\n",
    "                            ninth batch games is 248\n",
    "                            10th batch games is 288\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "major-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = DataBase()\n",
    "db_path = \"/storage/replay_buffer\"\n",
    "model = ZeroTTT(brain_path=None, opt_path=None, lr=lr, weight_decay=weight_decay, board_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "pharmaceutical-consciousness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183432\n"
     ]
    }
   ],
   "source": [
    "print(model.get_parameter_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "sensitive-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_paths = [os.path.join(db_path, \"states\", name) for name in sorted(os.listdir(\"/storage/replay_buffer/states\"))]\n",
    "policy_paths = [os.path.join(db_path, \"policy_labels\", name) for name in sorted(os.listdir(\"/storage/replay_buffer/policy_labels\"))]\n",
    "value_paths = [os.path.join(db_path, \"value_labels\", name) for name in sorted(os.listdir(\"/storage/replay_buffer/value_labels\"))]\n",
    "\n",
    "names = list(zip(state_paths, policy_paths, value_paths))\n",
    "filtered_names = []\n",
    "\n",
    "for i in range(len(names)):\n",
    "    index = int(names[i][0].split(\"_\")[-1][:-4])\n",
    "    if index > -1:\n",
    "        filtered_names.append(names[i])\n",
    "names = filtered_names\n",
    "#test_set = names[-2:]\n",
    "#names = names[:-2]\n",
    "test_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "corresponding-webmaster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "three-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss(model):\n",
    "    model.brain.eval()\n",
    "    total_p_loss = 0.0\n",
    "    total_v_loss = 0.0\n",
    "    for s, p, v in test_set:\n",
    "        batch_sts, batch_pls, batch_vls = database.prepare_batches(batch_size=batch_size, from_memory_paths=(s, p, v))\n",
    "        for b_nr in range(len(batch_sts)):\n",
    "            batch_st, batch_pl, batch_vl = batch_sts[b_nr], batch_pls[b_nr], batch_vls[b_nr]\n",
    "            \n",
    "            batch_pl = torch.from_numpy(batch_pl).to(model.device)\n",
    "            batch_vl = torch.from_numpy(batch_vl).float().to(model.device)\n",
    "            prob, val = model.predict(batch_st, interpret_policy=False)\n",
    "            val = val.flatten()\n",
    "\n",
    "            p_loss = model.policy_loss(prob, batch_pl)\n",
    "            v_loss = model.value_loss(val, batch_vl)\n",
    "        \n",
    "            total_p_loss += p_loss.item()\n",
    "            total_v_loss += v_loss.item()\n",
    "    return total_p_loss/(len(batch_sts)*len(test_set)), total_v_loss/(len(batch_sts)*len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "progressive-delaware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 train policy loss: 4.169333491732179 | train value loss: 0.542679855262395\n",
      "Saving brain...\n"
     ]
    }
   ],
   "source": [
    "train_policy_losses = []\n",
    "train_value_losses = []\n",
    "test_policy_losses = []\n",
    "test_value_losses = []\n",
    "for e in range(epochs):\n",
    "    model.brain.train()\n",
    "    cumulative_policy_epoch_loss = 0.0\n",
    "    cumulative_value_epoch_loss = 0.0\n",
    "    for s_name, p_name, v_name in names:\n",
    "        batch_sts, batch_pls, batch_vls = database.prepare_batches(batch_size=batch_size, from_memory_paths=(s_name, p_name, v_name))\n",
    "        for b_nr in range(len(batch_sts)):\n",
    "            model.optimizer.zero_grad()\n",
    "            batch_st, batch_pl, batch_vl = batch_sts[b_nr], batch_pls[b_nr], batch_vls[b_nr]\n",
    "            \n",
    "            batch_pl = torch.from_numpy(batch_pl).to(model.device)\n",
    "            batch_vl = torch.from_numpy(batch_vl).float().to(model.device)\n",
    "            prob, val = model.predict(batch_st, interpret_output=False)\n",
    "            val = val.flatten()\n",
    "\n",
    "            p_loss = model.policy_loss(prob, batch_pl)\n",
    "            v_loss = model.value_loss(val, batch_vl)\n",
    "                        \n",
    "            cumulative_policy_epoch_loss += p_loss.item()\n",
    "            cumulative_value_epoch_loss += v_loss.item()\n",
    "\n",
    "            loss = p_loss + v_loss\n",
    "            loss.backward()\n",
    "   \n",
    "            model.optimizer.step()\n",
    "    \n",
    "    # Loss on test set:\n",
    "    cumulative_policy_epoch_loss /= len(names)*len(batch_sts) # div by batch count\n",
    "    cumulative_value_epoch_loss /= len(names)*len(batch_sts)\n",
    "    # test_epoch_policy_loss, test_epoch_value_loss = test_loss(model)\n",
    "    print(f\"Epoch #{e} train policy loss: {cumulative_policy_epoch_loss} | train value loss: {cumulative_value_epoch_loss}\")\n",
    "    # print(f\"Test policy loss: {test_epoch_policy_loss} | Test value loss: {test_epoch_value_loss}\")\n",
    "    train_policy_losses.append(cumulative_policy_epoch_loss)\n",
    "    train_value_losses.append(cumulative_value_epoch_loss)\n",
    "    # test_policy_losses.append(test_epoch_policy_loss)\n",
    "    # test_value_losses.append(test_epoch_value_loss)\n",
    "    # Checkpoint:\n",
    "    model.save_brain(\"trained_model_0\", \"trained_opt_state_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_value_losses)\n",
    "plt.plot(test_value_losses)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_policy_losses)\n",
    "plt.plot(test_policy_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
